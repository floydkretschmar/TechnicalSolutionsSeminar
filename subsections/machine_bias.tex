The article \enquote{Machine Bias} by Julia Angwin and Jeff Larson describes the findings 
of ProPublica with regards to the risk assessment tool COMPAS. Their findings seem to confirm
a lot of the problems described by \cite{Barocas.2016}, \cite{barocas-hardt-narayanan} and
\cite{Berk.2018}. First of all, they found problems with regards to overall accuracy of 
predicting future crimes, which was \enquote{only 61 percent [...] for [committing] any 
subsequent crimes withing two years.} \cite{machinebias}. Moreover they found that false 
positive rates for black defendants was \enquote{almost twice the rate as white defendants}
\cite{machinebias} and \enquote{White defendants were mislabled as low risk more often 
than black defendants} \cite{machinebias}.

The authors go on to explain history of criminal risk assessment in the US overall and 
explain the motivation for deploying algorithmic risk assessment: \enquote{If computers
could accurately predict [...] new crimes, the criminal justice system could be fairer 
and more selective about who is incarcerated and for how long.} \cite{machinebias}. The 
article then explores in more detail, how modern algorithmic tools have performed in 
making risk predictions and how these predictions differ among protected groups.

In the final part of the article the authors describe how algorithmically generated risk 
scores generated are being used today. They explain how these systems are deployed in 
various US states and jurisdictions and which degree of impact they have had on decision 
making processes of judges, prosecutes, defenders and other parts of the criminal justice 
system.