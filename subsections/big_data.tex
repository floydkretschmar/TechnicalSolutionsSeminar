The whitepaper "Big Data’s Disparate Impact" \cite{Barocas.2016} by Solon Barocas and 
Andrew D. Selbst is separated into three main parts, which deal with slightly different 
topics regarding fairness in machine learning, in particular data mining. The first part 
focuses on the various ways through which data mining can discriminitate certain classes, 
while the second and third part discuss the liability issue of discrimination in data 
mining for the american title VII (equal employment opportunity) \cite{titleVII} of the 
civil rights act and the difficulty for future legal reforms.  According to their studies, there are five main structures in data mining which can 
cause discrimination for certain classes. In particular, these are the "definition of 
the target variable and class labels" (I), "training data" (II), "feature selection" (III), 
"proxies" (IV) and "masking" (V) \cite{Barocas.2016}. All of these topics have already 
been clarified and described regarding their extent in section \ref{sec:introduction} and 
therefore won't need special attention here.

In the american civil rights act, especially in title VII, there are two presented cases 
for discrimination, namely "disparate treatment" and "disparate impact", which also find 
usage in the presented whitepaper. While disparate treatment describes an uneven behavior 
towards a certain person due to a particular characteristic (e.g. gender, race or 
religion), disparate impact represents a neutral rule which treats everyone equally in 
form, but has a damaging effect on a subset of people with such a certain characteristic \cite{titleVII}.

In their whitepaper Barocas and Selbst argue that formal liability in disparate treatment 
doesn't correspond to any special step within data mining and that using a protected 
class as an input for any classificatory model should be a legal harm, because this 
process corresponds to the employer classifying and differentiating potential hires 
according to exactly this protected class \cite{Barocas.2016}. They also show that the 
disparate treatment either occurs at the decision to apply  a biased predictive model 
or when the biased result gets used for the ultimate hiring decision and draw the 
conclusion that the disparate treatment doctrine doesn't regulate discriminatory 
data mining to a satisfying extent \cite{Barocas.2016}. 

While considering the disparate impact doctrine, the authors state that in such a case 
the plaintiff must prove that "a particular facially neutral employment practice causes 
a disparate impact with respect to a protected class" \cite{Barocas.2016} 
\cite{titleVII}.\footnote{ 42 U.S.C.§2000e-2(k)(1)(A) } In response, the defendant-employer 
is then allowed to justify the challanged practice by showing the job relation and 
business necessity.\footnote{ \textit{Id.} } The plantiff then still has the chance 
to show that an alternative, less discriminatory employment practice could have been 
used instead \cite{Barocas.2016}. For the case of data mining this means that liability 
regarding disparate impact can be caused by using a non job related target variable 
\cite{Barocas.2016}. As soon as the target variable is shown to be job related, there 
are two questions which need to be answered. First, whether or not the model is 
predictive of the trait and secondly if the model with statistical significance 
predicts what it is supposed to predict \cite{Barocas.2016}. Barocas and Selbst 
also explain that it is hard to know which features would make an existing model more
or less discriminatory and therefore proving that a less discriminatory alternative 
would exist becomes a very hard task to solve \cite{Barocas.2016}.

The presented \textit{internal} issues with data mining are fundamental questions that need to be addressed or can't be solved properly. For example, the target variable will always inherent certain kinds of biases, because a target variable must contain judgments about what is really important in the presented problem \cite{Barocas.2016}. Additionally, a solution to the issues with training data labeling need to compromise between forbidding employers from using past discrimination and allowing them to use historical data of good employees \cite{Barocas.2016}. For skewed data sets the employer needs to recognize the type of bias, have access to the underlying data and needs the possibility to collect more data  \cite{Barocas.2016}. Otherwise, oversampling underrepresented communities can clear up some of the bias \cite{Barocas.2016}. Statistical discrimination in the area of feature selection is avoidable if there is the possibility to gather additional or more granular data \cite{Barocas.2016}. Otherwise minimizing the error rate between groups can help to improve this aspect \cite{Barocas.2016}. Lastly, for proxies there needs to be a threshold which defines when a correlation between an attribute and class membership becomes worrisome, as well as when it is sufficiently relevant despite being highly correlated to class membership \cite{Barocas.2016}.

