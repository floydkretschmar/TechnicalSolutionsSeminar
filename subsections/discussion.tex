From the presented papers as well as additional literature research for the missing EU law structures we conclude that currently the discrimination law structures aren't well prepared for the challenges which are brought up by automated decision making systems \cite{Barocas.2016, barocas-hardt-narayanan, automatedDsicrimination}. This applies for article 14 of the European Convention on Human Rights \cite{EU14}, the article 21 of the Charter of Fundamental rights of the EU \cite{EU21}, as well as title VII \cite{titleVII} of the american civil rights act  \cite{Barocas.2016, automatedDsicrimination}. Moreover, it is going to be interesting how the European General Data Protection Regulation (GDPR) will influence this subject in the upcoming years \cite{automatedDsicrimination, Singh}.

All of the presented papers describe issues within the machine learning process which are hard if not impossible to solve for future liability improvements and the authors of \cite{ Barocas.2016, Berk.2018} argue that discussion about fairness in machine learning is fundamentally a discussion about tradeoffs. Berk et al. state that the problem at the heart of fairness in machine learning is the difference
in base rate across protected groups which \enquote{can cascade through fairness assessments
and lead to difficult tradoffs.} \cite{Berk.2018}. In terms of discussing these tradeoffs, the authors make multiple suggestions. First, the tradeoffs need to be explicitly represented and available as tuning parameters. Secondly, future measures of fairness should be formulated in such a way, that trade-offs can be made with them. And thirdly, the determination of tradeoffs should ultimately fall in the hands of the stakeholders.

In this field there have also been various works like \cite{DBLP:conf/kdd/FeldmanFMSV15, isabel02, isabel01} and many more for detecting and removing discrimination in automated decision making systems. 