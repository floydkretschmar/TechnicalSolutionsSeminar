[Include additional work like: \cite{isabel01} (Isabel) \cite{isabel02} (Isabel) \cite{Singh} \cite{automatedDsicrimination}]

From the presented papers as well as additional literature research we conclude that currently the discrimination law structures aren't well prepared for the challenges which are brought up by automated decision making systems \cite{Barocas.2016, barocas-hardt-narayanan, automatedDsicrimination}. This applies for article 14 of the European Convention on Human Rights \cite{EU14}, the article 21 of the Charter of Fundamental rights of the EU \cite{EU21}, as well as title VII \cite{titleVII} of the american civil rights act  \cite{Barocas.2016, automatedDsicrimination}. Moreover, it is going to be interesting how the European General Data Protection Regulation (GDPR) will influence this subject in the upcoming years \cite{automatedDsicrimination, Singh}.

All of the presented papers describe issues within the machine learning process and the authors of \cite{ Barocas.2016, Berk.2018} argue that the discussion about fairness in machine learning is fundamentally a discussion about tradeoffs. Berk et al. state that the problem at the heart of fairness in machine learning is the difference
in base rate across protected groups which \enquote{can cascade through fairness assessments
and lead to difficult tradoffs.} \cite{Berk.2018}. In terms of discussing these tradeoffs, the authors make multiple
suggestions:
\begin{itemize}
    \item The tradeoffs need to be explicitly represented and available as tuning parameters.
    \item Future measures of fairness should be formulated in such a way, that tradeoffs can
    be made with them.
    \item The determination of tradeoffs should ultimately fall in the hands of the stakeholders.
\end{itemize}
As a final point \cite{ Barocas.2016, Berk.2018} state that any solution will likely not come fast and no singular
solution will be able to reverse longstanding, institutionalized inequality.