In the paper \enquote{Fairness in Criminal Justice Risk Assessments} \cite{Berk.2018} 
the authors explore the concept of fairness in the technical and mathematical context. 
More specifically they are interested to find a way, a notion of fairness can be 
operationalized in the context of machine learning and how it applies to the specific
case of criminal justice risk assessment. The following discussions of fairness are based 
on the statistical framework presented in section \ref{sec:introduction}. For ease of 
exposition the paper limits itself to the case where $Y$ and $\hat{Y}$ are binary.

%Their work can be roughly subdivided into four major parts. In the first part the authors 
%introduce the statistical framework as well as the fundamental mathematical terminology 
%from which they try to derive their notions of fairness. In the second part of the paper
%they go on to define six types of algorithmic fairness, based on the statistical foundations 
%laid out before. Then follows a discussion of the tradeoffs that have to be made when trying
%to achieve different notions of fairness. Both the tradeoff with regards to the overall
%prediction accuracy as well as the tradeoff between different kinds of fairness is 
%discussed. Finally the authors introduce a range of potential solutions to enforce fairness
%in machine learning algorithms.

In the first parts of the paper, the underlying statistical framework is defined. The 
fairness definitions proposed are based on the accuracy measurements defined by the 
confusion matrix \footnote{ A full explanation of the confusion matrix can be found in 
section \ref{sec:appendix}}. More specifically the authors define fairness as an equality 
of accuracy across all protected group categories. Imposing this equality constraint for 
each kind of accuracy defined by the confusion matrix, leads to the following definitions 
of fairness: 

\begin{enumerate}
    \item \textbf{Overall accuracy equality:} equal probability of correct classification 
    $\frac{t_p + t_n}{N}$ 
    \item \textbf{Statistical parity:} equal probability of predicting failure/success 
    $\frac{t_p + f_p}{N}$ or $\frac{t_n + f_n}{N}$
    \item \textbf{Conditional procedure accuracy equality:} equal probability of correct 
    classification, given the actual outcome: $\frac{t_p}{t_p + f_n}$ or 
    $\frac{t_n}{t_n + f_p}$
    \item \textbf{Conditional use accuracy equality:} equal probability of an actual 
    outcome, given the prediction: $\frac{t_p}{t_p + f_p}$ or $\frac{t_n}{t_n + f_n}$
    \item \textbf{Treatment equality:} equal ratio between false negatives and positives: 
    $\frac{f_p}{f_n}$ and $\frac{f_n}{f_p}$
    \item \textbf{Total fairness:} All previously notions of fairness are achieved 
    simultaneously
\end{enumerate}

%\begin{itemize}
%    \item \textbf{Base Rate:} probability of actual failures/successes $\frac{t_p + f_n}{N}$ and 
%    $\frac{t_n + f_p}{N}$
%    \item \textbf{Prediction Distribution:} probability of predicted failures/successes 
%    $\frac{t_p + f_p}{N}$ and $\frac{t_n + f_n}{N}$
%    \item \textbf{Overall Procedure Accuracy:} probability of correct classification: 
%    $\frac{t_p + t_n}{N}$
%    \item \textbf{Conditional Procedure Accuracy:} probability of correct classification, given 
%    the actual outcome: $\frac{t_p}{t_p + f_n}$ and $\frac{t_n}{t_n + f_p}$
%    \item \textbf{Conditional Use Accuracy:} probability of an actual outcome, given the 
%    prediction: $\frac{t_p}{t_p + f_p}$ and $\frac{t_n}{t_n + f_n}$
%    \item \textbf{Cost Ratio:} ratio between false negatives and positives: $\frac{f_p}{f_n}$ and 
%    $\frac{f_n}{f_p}$
%\end{itemize}

For the discussion of fairness it is assumed that $f(L,S) = \hat{f}(L,S)$ as to no 
conflate discussions about fairness and accuracy. But the notions about accuracy discussed 
in section \ref{sec:introduction} also hold true for the probabilities defined by the
confusion matrix of $Y$ and $\hat{Y}$. 

In the next part follows a discussion of necessary tradeoffs between accuracy and 
fairness as well as between different kind of fairness. The paper states that \enquote{
\dots excluding $S$ will reduce accuracy. Any procedure that even just discounts the role 
of $S$ will lead to less accuracy.}\cite{Berk.2018} The authors also explore the conflict 
between conditional use and procedure accuracy equality by citing the following impossibility 
theorem: \enquote{When the base rates\footnote{ proportion of actual failures/successes 
$\frac{t_p + f_n}{N}$ or $\frac{t_n + f_p}{N}$} differ by protected group and when there 
is not separation\footnote{ separation = \enquote{perfectly accurate classification is
possible}\cite{Berk.2018}}, one cannot have both conditional use accuracy and equality in 
the false negative and false positive rates.}\cite{DBLP:journals/corr/KleinbergMR16}
\cite{Chouldechova2017FairPW}. The authors suggest, that \enquote{the key tradeoff will be 
between the false positive and false negative rates on the one hand and the conditional use 
accuracy on the other.}\cite{Berk.2018}

In the final parts of the paper multiple approaches for solving the issue of fairness in 
machine learning are introduced and briefly discussed. The paper explores three different
main strategies, which can be combined.
\begin{enumerate}
    \item \textbf{Pre-Processing} is the elimination of sources of unfairness in the data before 
    formulating $h(L,S)$. Examples include the removal of linear dependencies between $L$ and
    $S$, the rebalancing of base rates or the random transformation of predictors, such that
    $P(Y,L,S)$ is less dependent on $S$.
    \item \textbf{In-Processing} means including the adjustments for fairness in the process 
    of constructing $h(L,S)$. One example of this, is enforcing more fair results according 
    to the defined notions of fairness, if the initial prediction $\hat{Y}$ had substantial 
    uncertainty.
    \item In \textbf{Post-Processing} $h(L,S)$ is applied first, and its results are adjusted
    afterwards to account for fairness. A possible approach for Post-Processing is the 
    reassignment of class labels after classification with the goal of minimizing 
    classification errors subject to a particular fairness constraint.
\end{enumerate}

The authors note, that all the corrections for fairness presented are themselves agnostic 
about \enquote{what the target outcome for fairness should be.}\cite{Berk.2018} They 
argue that a discussion about the benchmark according to which equality is achieved, is just as
important, as it makes the determination of tradeoffs more complicated.