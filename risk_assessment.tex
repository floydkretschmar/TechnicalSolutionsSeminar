In the paper \enquote{Fairness in Criminal Justice Risk Assessments} \cite{Berk.2018} 
the authors explore the concept of fairness in the technical and mathematical context. 
More specifically they are interested to find a way, a notion of fairness can be 
operationalized in the context of machine learning and how it applies to the specific
case of criminal justice risk assessment. 

%Their work can be roughly subdivided into four major parts. In the first part the authors 
%introduce the statistical framework as well as the fundamental mathematical terminology 
%from which they try to derive their notions of fairness. In the second part of the paper
%they go on to define six types of algorithmic fairness, based on the statistical foundations 
%laid out before. Then follows a discussion of the tradeoffs that have to be made when trying
%to achieve different notions of fairness. Both the tradeoff with regards to the overall
%prediction accuracy as well as the tradeoff between different kinds of fairness is 
%discussed. Finally the authors introduce a range of potential solutions to enforce fairness
%in machine learning algorithms.

The discussions in \cite{Berk.2018} are based on the statistical framework presented in section 
\ref{sec:introduction}. For ease of exposition the paper limits itself to the case where $Y$ and 
$\hat{Y}$ are binary. The authors define fairness based on the accuracy measurements defined 
by the confusion matrix\footnote{A full explanation of the confusion matrix can be found in 
section \ref{sec:appendix}}. More specifically they define fairness as an equality of accuracy 
across all protected group categories. Imposing this equality constraint for each kind of accuracy
defined by the confusion matrix, leads to the following definitions of fairness: 

\begin{enumerate}
    \item \textbf{Overall accuracy equality:} equal probability of correct classification 
    $\frac{t_p + t_n}{N}$ 
    \item \textbf{Statistical parity:} equal probability of predicting failure/success 
    $\frac{t_p + f_p}{N}$ or $\frac{t_n + f_n}{N}$
    \item \textbf{Conditional procedure accuracy equality:} equal probability of correct classification, given 
    the actual outcome: $\frac{t_p}{t_p + f_n}$ or $\frac{t_n}{t_n + f_p}$
    \item \textbf{Conditional use accuracy equality:} equal probability of an actual outcome, given the 
    prediction: $\frac{t_p}{t_p + f_p}$ or $\frac{t_n}{t_n + f_n}$
    \item \textbf{Treatment equality:} equal ratio between false negatives and positives: $\frac{f_p}{f_n}$ and 
    $\frac{f_n}{f_p}$
    \item \textbf{Total fairness:} All previously notions of fairness are achieved simultaneously
\end{enumerate}

%\begin{itemize}
%    \item \textbf{Base Rate:} probability of actual failures/successes $\frac{t_p + f_n}{N}$ and 
%    $\frac{t_n + f_p}{N}$
%    \item \textbf{Prediction Distribution:} probability of predicted failures/successes 
%    $\frac{t_p + f_p}{N}$ and $\frac{t_n + f_n}{N}$
%    \item \textbf{Overall Procedure Accuracy:} probability of correct classification: 
%    $\frac{t_p + t_n}{N}$
%    \item \textbf{Conditional Procedure Accuracy:} probability of correct classification, given 
%    the actual outcome: $\frac{t_p}{t_p + f_n}$ and $\frac{t_n}{t_n + f_p}$
%    \item \textbf{Conditional Use Accuracy:} probability of an actual outcome, given the 
%    prediction: $\frac{t_p}{t_p + f_p}$ and $\frac{t_n}{t_n + f_n}$
%    \item \textbf{Cost Ratio:} ratio between false negatives and positives: $\frac{f_p}{f_n}$ and 
%    $\frac{f_n}{f_p}$
%\end{itemize}
