\begin{frame}{The Machine Learning Framework: How is it unfair? \cite{Barocas.2016}}
    \textbf{Target Variable or class Labels:} \\
    \begin{itemize}
        \item target variable defines what data miners are looking for
        \item class labels divide all possible values of the target variable into mutually exclusive categories
    \end{itemize}
    \begin{block}{\textbf{Unfairness happening through:}}
    \begin{itemize}
        \item Subjective process
        \item Could be unintentionally be parsed in a way which systematically disadvantages protected classes
    \end{itemize}
        \end{block}
\end{frame}

\begin{frame}{The Machine Learning Framework: How is it unfair? \cite{Barocas.2016}}
    \textbf{Training Data:}
    \begin{itemize}
        \item Basis of used algorithms
    \end{itemize}
    \begin{block}{\textbf{Unfairness happening through:}}
    \begin{itemize}
        \item Biased training data leads to discriminatory models
        \item 1. Cases in which prejudice has played some role
        \item 2. Biased sample of the population (under-/overrepresenttation)
    \end{itemize}
        \end{block}
\end{frame}

\begin{frame}{The Machine Learning Framework: How is it unfair? \cite{Barocas.2016}}
    \textbf{Feature Selection:}
    \begin{itemize}
        \item Observed attributes
    \end{itemize}
    \begin{block}{\textbf{Unfairness happening through:}}
    \begin{itemize}
        \item Factors that better account for a protected class are not well represented in the set of selected features 
        \item e.g. hiring descisions, enourmous weight to reputation of college
    \end{itemize}
        \end{block}
\end{frame}

\begin{frame}{The Machine Learning Framework: How is it unfair? \cite{Barocas.2016}}
    \textbf{Proxies:}
\end{frame}

\begin{frame}{The Machine Learning Framework: How is it unfair? \cite{Barocas.2016}}
    \textbf{Masking:}
\end{frame}